# Musubi Tuner (HunyuanVideo) LoRA Training - Default Configuration
# This file contains the default values for HunyuanVideo LoRA training
# Based on HunyuanVideo specific requirements and examples

# Model Settings (HunyuanVideo specific)
dit_dtype = "bfloat16"
vae_dtype = "float16"
text_encoder_dtype = "float16"
fp8_llm = false
fp8_base = false
vae_tiling = false
vae_chunk_size = 0
vae_spatial_tile_sample_min_size = 256
blocks_to_swap = 0
img_in_txt_in_offloading = false
guidance_scale = 1.0

# Flow Matching Settings (HunyuanVideo updated defaults - as per README examples)
timestep_sampling = "shift"
discrete_flow_shift = 7.0
weighting_scheme = "none"
logit_mean = 0.0
logit_std = 1.0
mode_scale = 1.29
show_timesteps = ""

# Training Settings (HunyuanVideo defaults)
sdpa = false
flash_attn = false
sage_attn = false
xformers = false
split_attn = false
max_train_steps = 1600
max_train_epochs = 1
max_data_loader_n_workers = 8
persistent_data_loader_workers = false
seed = 0
gradient_checkpointing = false
gradient_accumulation_steps = 1

# Optimizer Settings (HunyuanVideo updated defaults - as per README examples)
optimizer_type = "adamw8bit"
learning_rate = 2e-4
max_grad_norm = 1.0
lr_scheduler = "constant"
lr_warmup_steps = 0
lr_decay_steps = 0
lr_scheduler_num_cycles = 1
lr_scheduler_power = 1.0
lr_scheduler_timescale = 0
lr_scheduler_min_lr_ratio = 0.0
lr_scheduler_type = ""
lr_scheduler_args = ""

# Network Settings (HunyuanVideo defaults)
no_metadata = false
network_weights = ""
network_module = "networks.lora"
network_dim = 1
network_alpha = 1.0
network_dropout = 0.0
network_args = ""
training_comment = ""
dim_from_weights = false
scale_weight_norms = 0.0
base_weights = ""
base_weights_multiplier = 1.0

# Save/Load Settings
output_dir = ""
output_name = ""
resume = ""
save_every_n_epochs = 0
save_every_n_steps = 0
save_last_n_epochs = 0
save_last_n_epochs_state = 0
save_last_n_steps = 0
save_last_n_steps_state = 0
save_state = false
save_state_on_train_end = false

# Caching Settings - Latents (HunyuanVideo)
caching_latent_device = "cuda"
caching_latent_batch_size = 1
caching_latent_num_workers = 1
caching_latent_skip_existing = false
caching_latent_keep_cache = false
caching_latent_debug_mode = ""
caching_latent_console_width = 80
caching_latent_console_back = ""
caching_latent_console_num_images = 0

# Caching Settings - Text Encoder (HunyuanVideo dual encoders)
caching_teo_text_encoder1 = ""
caching_teo_text_encoder2 = ""
caching_teo_text_encoder_dtype = "float16"
caching_teo_fp8_llm = false
caching_teo_device = "cuda"
caching_teo_batch_size = 1
caching_teo_num_workers = 1
caching_teo_skip_existing = false
caching_teo_keep_cache = false

# Accelerate Launch Settings - Updated defaults
mixed_precision = "bf16"
multi_gpu = false           # Enable distributed multi-GPU training
gpu_ids = ""                # GPU IDs for distributed training (e.g., "0,1,2,3")
num_processes = 1           # Number of processes for distributed training
num_machines = 1            # Number of machines for distributed training
num_cpu_threads_per_process = 2  # CPU threads per process
main_process_port = 0       # Port for distributed training communication (0 = auto)
dynamo_backend = "no"       # no = disabled (recommended). Use inductor for PyTorch 2.0+ optimization
dynamo_mode = ""            # Empty = default. Options: "default", "reduce-overhead", "max-autotune"
dynamo_use_fullgraph = false   # Use fullgraph mode for dynamo
dynamo_use_dynamic = false     # Use dynamic mode for dynamo
extra_accelerate_launch_args = ""  # Additional accelerate launch arguments

# Logging Settings
logging_dir = ""
log_with = ""
log_prefix = ""
log_tracker_name = ""
wandb_run_name = ""
log_tracker_config = ""
wandb_api_key = ""
log_config = false

# DDP Settings
ddp_timeout = 0
ddp_gradient_as_bucket_view = false
ddp_static_graph = false

# Sample Generation
sample_every_n_steps = 0
sample_every_n_epochs = 0
sample_at_first = false
sample_prompts = ""

# Metadata Settings
metadata_author = ""
metadata_description = ""
metadata_license = ""
metadata_tags = ""
metadata_title = ""

# HuggingFace Settings
huggingface_repo_id = ""
huggingface_token = ""
huggingface_repo_type = ""
huggingface_repo_visibility = ""
huggingface_path_in_repo = ""
save_state_to_huggingface = false
resume_from_huggingface = false
async_upload = false